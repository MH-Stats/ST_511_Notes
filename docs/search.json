[
  {
    "objectID": "ST_511_Notes.html",
    "href": "ST_511_Notes.html",
    "title": "ST 511 Notes",
    "section": "",
    "text": "Replace file path with an empty folder path one on your end and change the url to the one you want to copy:\n\ngit clone https://github.com/MH-Stats/Math_Notes.git \"C:\\Users\\mheinen\\Documents\\Repos\\SSH\\_Test\"\n\nHow to pull files from GitHub repo:\n\ngit pull origin main\n\nHow to commit and upload changes using gitbash:\n\nchange lib in gitbash cd C:/Users/mheinen/Documents/Repos/SSH_Test\n\nCheck untracked files:\n\ngit status\n\nStage specific file:\n\ngit add filename\n\nStage all files:\n\ngit add .\n\nCommit changes: - git commit -m \"Add your message here\"\nPush changes to the repo (change main to branch if using a different one): - git push origin main"
  },
  {
    "objectID": "ST_511_Notes.html#gitgithub",
    "href": "ST_511_Notes.html#gitgithub",
    "title": "ST 511 Notes",
    "section": "",
    "text": "Replace file path with an empty folder path one on your end and change the url to the one you want to copy:\n\ngit clone https://github.com/MH-Stats/Math_Notes.git \"C:\\Users\\mheinen\\Documents\\Repos\\SSH\\_Test\"\n\nHow to pull files from GitHub repo:\n\ngit pull origin main\n\nHow to commit and upload changes using gitbash:\n\nchange lib in gitbash cd C:/Users/mheinen/Documents/Repos/SSH_Test\n\nCheck untracked files:\n\ngit status\n\nStage specific file:\n\ngit add filename\n\nStage all files:\n\ngit add .\n\nCommit changes: - git commit -m \"Add your message here\"\nPush changes to the repo (change main to branch if using a different one): - git push origin main"
  },
  {
    "objectID": "ST_511_Notes.html#random-variable",
    "href": "ST_511_Notes.html#random-variable",
    "title": "ST 511 Notes",
    "section": "Random Variable",
    "text": "Random Variable\nRandom variables (RV) are ways to map random or stochastic process to numbers, which is a mathematical object defined as a family of random variables in a probability space, with the most common index fro the family being time. They are the building blocks of probability distributions, and a probability distribution describes the set of all possible values a RV can take and the probability of each value occurring.\n\nCan be discrete or continuous\nNormally represented by uppercase letters like \\(X\\)\nUseful when you know the exact population distribution, as you can use it to calculate probabilities, as an example you know the probability of rolling a 1 through 6 on a dice."
  },
  {
    "objectID": "ST_511_Notes.html#probability-and-sampling-distribution",
    "href": "ST_511_Notes.html#probability-and-sampling-distribution",
    "title": "ST 511 Notes",
    "section": "Probability and Sampling Distribution",
    "text": "Probability and Sampling Distribution\nThe probability distribution of a population directly influences the sampling distribution of a statistic drawn from that population. A sampling distribution is essentially the probability distribution of a statistic from all possible samples of a given size.\nThe spread of the sampling distribution is,\n\\[\\frac{\\sigma}{\\sqrt{n}}\\]\nSince we normally only get a single sample, we assume that,\n\\[\\bar{x} \\approx N(\\mu,\\frac{\\sigma}{\\sqrt{n}})\\]\n\nWe estimate \\(\\mu\\) with \\(\\bar{x}\\) and \\(\\sigma\\) with \\(s\\)\n\n\nNormality\nIf the probability distribution is normal, the sampling distribution will also be normal.\n\n\nThe Central Limit Theorem\nFor large independent samples (n&gt;30), regardless of of what the population distribution looks like, the sampling distribution of the sample mean will be normally distributed with mean \\(\\mu\\) and standard error of \\(\\frac{\\sigma}{\\sqrt{n}}\\)\nZ and t-test assume the sampling distribution is normally distributed. This is also why:\n\n\\(\\bar{x}\\) is unbiased estimator for \\(\\mu\\)\n\n\n\nNotation\nTo write the mean and standard deviation in a distribution you can write it as,\n\\[X \\sim N(\\mu, \\sigma)\\]\nWhen writing out the probability you can write it as,\n\\[P(X&gt;x) \\qquad \\text{where } X \\text{ is the random variable, } x \\text{ is the value, and &gt; is greater than (can be &gt;, &lt;, =, etc.)}\\]"
  },
  {
    "objectID": "ST_511_Notes.html#finding-average-word-count-in-r",
    "href": "ST_511_Notes.html#finding-average-word-count-in-r",
    "title": "ST 511 Notes",
    "section": "Finding Average Word Count in R",
    "text": "Finding Average Word Count in R\n\nlibrary(stringr) #Used for str_split\n\n#Creating a character vector from the address \naddress &lt;- \"Four score and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal.\n\nNow we are engaged in a great civil war, testing whether that nation, or any nation so conceived and so dedicated, can long endure. We are met on a great battle-field of that war. We have come to dedicate a portion of that field, as a final resting place for those who here gave their lives that that nation might live. It is altogether fitting and proper that we should do this.\n\nBut, in a larger sense, we can not dedicate -- we can not consecrate -- we can not hallow -- this ground. The brave men, living and dead, who struggled here, have consecrated it, far above our poor power to add or detract. The world will little note, nor long remember what we say here, but it can never forget what they did here. It is for us the living, rather, to be dedicated here to the unfinished work which they who fought here have thus far so nobly advanced. It is rather for us to be here dedicated to the great task remaining before us -- that from these honored dead we take increased devotion to that cause for which they gave the last full measure of devotion -- that we here highly resolve that these dead shall not have died in vain -- that this nation, under God, shall have a new birth of freedom -- and that government of the people, by the people, for the people, shall not perish from the earth.\"\n\naddress_count &lt;- str_split(address, \"\\\\W+\") #Splits up the text by \" \" and removes white space and other non-letter chars from address\n\naddress_clean &lt;- address_count[[1]][-273] #Removes last element which was null aka \"\"\n\naddress_clean[1:20] #Using indexing to show a snippet of what the character vector now looks like\n\n [1] \"Four\"      \"score\"     \"and\"       \"seven\"     \"years\"     \"ago\"      \n [7] \"our\"       \"fathers\"   \"brought\"   \"forth\"     \"on\"        \"this\"     \n[13] \"continent\" \"a\"         \"new\"       \"nation\"    \"conceived\" \"in\"       \n[19] \"Liberty\"   \"and\"      \n\n\n\navg_length &lt;- mean(nchar(address_clean)) #nchar is a built in R function to tell you character length\n\n#Paste allows you to put multiple inputs into a function like print()\nprint(paste(\"The average word length in the Gettysburg Address is\", avg_length))\n\n[1] \"The average word length in the Gettysburg Address is 4.22426470588235\"\n\n\n\nset.seed(24) #Setting seed\n\naddress_sample &lt;- sample(address_clean, 10) #Sampling 10 words from the cleaned address vector\n\navg_sample_length &lt;- mean(nchar(address_sample))\n\nprint(paste(\"The average word length in the sample of the Gettysburg Address is\", avg_sample_length))\n\n[1] \"The average word length in the sample of the Gettysburg Address is 5.9\""
  },
  {
    "objectID": "ST_511_Notes.html#hypothesis-testing",
    "href": "ST_511_Notes.html#hypothesis-testing",
    "title": "ST 511 Notes",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\nAssumptions\nMust check that the data has two thing:\n\nIndependence\nNormality\n\nData must be “normal” meaning the sample size n must satisfy two conditions:\n\n\\(n * \\pi_0\\geq10\\)\n\\(n * (1-\\pi_0)\\geq10\\)\n\n\n\n\n\nZ-distribution\nZ-score is a ratio of how the sample proportion differs from the hypothesized proportion (\\(p_0\\)) as compared to the expected variability of the \\(\\hat{p}\\) (sample proportion) values.\n\\[Z = \\frac{\\hat{p}-p_0}{\\sqrt{\\frac{p_0(1-p_0)}{n}}}\\]\n\nThe standard error, which is the the standard deviation of the sampling distribution comes from the above formula,\n\n\\[SE=\\sqrt{\\frac{p_0(1-p_0)}{n}}\\]\nWhen the null hypothesis is true and the conditions are met, Z has a standard normal distribution.\n\n\nNotation\nWhen writing null and alternative hypothesis follow the following syntax based on if your data is quantitative or categorical:\nQuantitative\n\nYour hypothesis are about a population mean\nUse \\(\\mu\\), as an example say your testing that the average height of a population is 64 inches, but you believe the true average height is greater than 64\n\n\\[\n\\begin{align*}\nH_0 &: \\mu = 64 \\\\\nH_a &: \\mu &gt; 64\n\\end{align*}\n\\]\nCategorical\n\nUse \\(\\pi\\) when your dealing with categorical data, as you are testing the hypothesis about a population proportion. As an example say you want to test if 25% of people prefer sleeping with their socks on.\n\n\\[\n\\begin{align*}\nH_0 &: \\pi = .25 \\\\\nH_a &: \\pi \\ne .25\n\\end{align*}\n\\]"
  },
  {
    "objectID": "ST_511_Notes.html#confidence-intervals",
    "href": "ST_511_Notes.html#confidence-intervals",
    "title": "ST 511 Notes",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\nThe goal of a confidence interval is to create a sampling distribution around our statistic, and use the standard error (sampling variability) to provide a range of values to estimate our population parameter.\nA confidence interval is a derived from the successes and failures of our random sample, while hypothesis testing is the successes and failures under the assumption of the null hypothesis.\n\nNotation\nFormula:\n\\[\\hat{p}\\pm MOE\\]\nMargin of Error (MOE):\n\\[z^* * \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\]\n\nThe square root in the formula is equal to \\(SE(\\hat{p})\\)\n\n\n\nExample\nWe want to estimate the number of NC State students that eat ice cream. Let’s assume that 100 NC State students were randomly sampled, and it turned out that \\(\\hat{p}=0.37\\). This is our best guess for π, which nobody thinks is actually correct.\nWe need to check for a couple things and note our variables of interest:\n\nIndependence\n\\(\\hat{p}=0.37\\)\n\\(n=100\\)\nSuccess/Failure:\n\n\\[SE(\\hat{p})=\\sqrt{\\frac{0.37(1-0.37)}{100}}=0.0482\\]\nNow we can calculate a 95% confidence interval:\n\\[0.37 \\pm 1.96 * 0.0428=(0.286, 0.454)\\]\nTwo ways to state in words:\n\nWe are 95% confident \\(\\pi\\) is within (0.286, 0.454).\nWe are 95% confident that the true proportion of NC State students who eat Howling Cow ice cream at dinner is between 0.286 and 0.454."
  },
  {
    "objectID": "ST_511_Notes.html#difference-in-proportions-test",
    "href": "ST_511_Notes.html#difference-in-proportions-test",
    "title": "ST 511 Notes",
    "section": "Difference In Proportions Test",
    "text": "Difference In Proportions Test\nAlso know as a two-proportion z-test. It is a z-test used to determine whether the difference between the proportions of two groups, coming from a binomial distribution is statistical significant.\n\nExample\nA pharmaceutical company is conducting a clinical trial to test the effectiveness of a new drug for a common illness. They randomly assign participants to one of two groups\nGroup 1 (New Drug): Out of 200 patients, 160 recovered from the illness.\nGroup 2 (Placebo): Out of 180 patients, 126 recovered from the illness.\nPerform a hypothesis test to determine if the proportion of patients who recovered is significantly different for the group that received the new drug\n\nNotation\n\\(H_o:\\pi_n-\\pi_p=0\\)\n\\(H_a:\\pi_n-\\pi_p&gt;0\\)\n\\(\\hat{p}_n - \\hat{p}_p=.1\\)\n\n\nAssumptions\n\nIndependence (observation level, not variables)\nSuccess-failure (sample size)\n\n\n\nYou need to check both \\(p_1\\) & \\(p_2\\) by using the pooled proportion multiplied by the sample size n for each sample.\n\nis \\(p_{pool} * n_1&gt;10\\)\nis \\((1-p_{pool})*n_1&gt;10\\)\nis \\(p_{pool}*n_2&gt;10\\)\nis \\((1-p_{pool})*n_2&gt;10\\)\n\n\n\n\nZ-test\n\n\\[\nZ=\\frac{.1-0}{\\sqrt{.744*.226(\\frac{1}{200}+\\frac{1}{180})}}=2.327\n\\]\n\n\nConclusion\nBecause or p-value is &lt; α, we reject the null hypothesis, and have strong evidence to conclude that the true proportion of patients who recovered from the illness taking the new drug is larger than those who took the placebo.\n\n\n\nConfidence Interval\n\nAssumptions\n\nIndependence\nSuccess-failure\n\n\\(\\hat{p}_n*n_1&gt;10\\)\n\n\\(.8*200&gt;10\\)\n\n\\((1-\\hat{p}_n)*n_1&gt;10\\)\n\n\\(.2*200&gt;10\\)\n\n\\(\\hat{p}_p*n_2&gt;10\\)\n\n\\(.7*180&gt;10\\)\n\n\\((1-\\hat{p}_p)*n_2&gt;10\\)\n\n\\(.3*180&gt;10\\)\n\n\n\n\n\nCI\n\n\\[\n\\sqrt{\\frac{.8*.2}{200}+\\frac{.7*.3}{180}}=0.00443\n\\]\n\n\n\\(.1+.00433*1.96=0.109\\)\n\\(.1-.00443*1.96=0.091\\)\n\nWe are 95% confident that the true proportion of patients who took the new drug and recovered is 0.091 to 0.109 HIGHER than the true proportion of patients who took the placebo and recovered.\n\n\n\nRelationship Between Z-test and CI\nWhen working with a two-tailed test, if you reject the null hypothesis at \\(\\alpha=0.05\\) level, we would expect our 95% CI to not include the null values.\nThe condition for rejecting the null hypothesis is that the absolute Z-statistic is greater than the critical value,\n\\[\n|Z_{stat}|&gt;Z_{crit}\n\\]\nWhich can be rearranged to show,\n\\[\n|\\hat{p}-p_0|&gt;Z_{crit}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n\\]\nFor quantitative variables, the key insight is that when you reject the null hypothesis, the distance between your statistic and the null value is larger than the margin of error of the confidence interval. Since the null value is a certain distance away from your sample proportion, and that distance is greater than the width of your confidence interval, the null value cannot possibly be inside the interval."
  },
  {
    "objectID": "ST_511_Notes.html#central-limit-theorem",
    "href": "ST_511_Notes.html#central-limit-theorem",
    "title": "ST 511 Notes",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem"
  },
  {
    "objectID": "ST_511_Notes.html#t-distribution",
    "href": "ST_511_Notes.html#t-distribution",
    "title": "ST 511 Notes",
    "section": "t-Distribution",
    "text": "t-Distribution\nSimilar to normal distribution, but has more uncertainty.\n\nThere are infinite t-distributions vs. the singular normal distribution\nt-distribution approach a normal distribution as you increase your degrees of freedom.\nUsed when the population standard deviation is unknown.\nUsed for quantitative data.\n\n\nDegrees of Freedom\nDF for a t-distribution refer to the number of independent pieces of information that available to estimate a parameter. In the context of the t-distribution, DF are a parameter that determines the shape of the distribution.\n\n\\(DF = n-1\\)\n\nDF correct for the uncertainty introduced when estimating the population standard deviation using the sample deviation when working with a quantitative variable.\nSince the standard error of the sampling distribution is known for hypothesis test with categorical data, derived from \\(\\pi_o\\) , we don’t correct with t, instead we use the standard normal distribution.\n\n\nConnecting the dots\nIf the hypothesis test leads you to reject the null hypothesis, it means that the distance between \\(\\bar{x}\\) and \\(u_o\\) is greater than the margin of error defined by the 95% CI.\n\n\n\nMathematical Proof\n\\[\n|\\frac{\\bar{x}_1-u_o}{SE}|&gt;t^*\n\\]\n\\[\n|\\bar{x}_1-u_o|&gt;t^**SE\n\\]\nThis inequality states that the distance from the center of the interval to the null value \\(μ_o\\) is greater than the margin of error. If the distance to \\(μ_o\\) is longer than the margin of error around the sample mean, the confidence interval must exclude \\(μ_o\\).\nThese two equations are mathematically equivalent, proving that rejecting \\(H_o\\) is the same as the confidence interval excluding the null value \\(u_o\\).\n\nFor a two-side test where \\(\\alpha\\) “matches” with our confidence interval you would get something like, \\(\\alpha=0.05\\) and a 95% CI.\n\nWith categorical data the difference in their numerical values is usually too small to break the equivalence."
  },
  {
    "objectID": "ST_511_Notes.html#t-test-math-notation",
    "href": "ST_511_Notes.html#t-test-math-notation",
    "title": "ST 511 Notes",
    "section": "T-Test Math Notation",
    "text": "T-Test Math Notation\n\n\nStandard Deviation\n\n\n\nBetween Groups\n\n\n\nConfidence Interval\nNeed to know the margin of error, which you should \\(\\pm\\) from your best guess."
  },
  {
    "objectID": "ST_511_Notes.html#anova",
    "href": "ST_511_Notes.html#anova",
    "title": "ST 511 Notes",
    "section": "ANOVA",
    "text": "ANOVA\nANOVA is an extension from difference in means, where the test statistic is the variability (variance) within group vs the variability (variance) across groups.\n\nHypothesis Notation:\nExample text comes from the plants dataset in R.\n\\[\nH_o: \\mu_c = \\mu_{trt1} = \\mu_{trt2}\n\\]\nIn words:\n\n\\(H_o:\\) The population mean weight of plants across the three groups are equal.\n\\(H_a:\\) At least one population mean plant weight is different across the three groups.\n\nNote: It is uncommon to write our the alternative hypothesis in notation, but the null is generally written in proper notation.\n\n\nAssumptions\nIndependence\n\nBetween groups\nAcross groups\n\nNormality\n\nCheck using quantiles and a Normal q-q plot to determine if the data is normally distributed.\n\nWe plot the values from our data set against what woe would expect the values to look like if they were normally distributed\nIf the dots roughly follow a straight line, we have evidence of normality.\n\n\n\n\nEqual variance\nGeneral rule: Is the largest sample standard deviation more than twice the smallest sample standard deviation?\n\n\nThe Math Behind ANOVA\n\\[\nF = \\frac{MS_{Between}}{MS_{Within}} \\\\\nF = \\frac{\\frac{SS_{Between}}{k-1}}{\\frac{SS_{Within}}{N-k}} \\\\\nSS_{Between} = \\sum^k_{i=1}n_i(\\bar{X}_i-\\bar{X}_{grand})^2 \\\\\nSS_{Within} = \\sum^k_{i=1}\\sum^{n_i}_{j=1}(X_{ij}-\\bar{X}_i)^2\n\\]\n\nSS means sum of squares\nF is our statistics\n\n\nDefinitions\nThe Sum of Squares (SS) is a measure of total variability (between or within)\nThe Mean Squares (MS) is an estimate (\\(s^2\\)) of the population variance (\\(\\sigma^2\\)), represented as,\n\\[\ns^2=\\frac{\\sum(x_i-\\bar{x})^2}{n-1}\n\\]\n\nThis is the \\(\\frac{SS}{df}\\)\n\n\n\n\nF Distribution\nA continuous, positively skewed probability distribution, characterized by two degrees of freedom (numerator and denominator) that determine its shape.\n\nAlways one tail no matter what\n\n\nDegrees of Freedom\nDegrees of Freedom (df) is the number of independent pieces of information used to calculate the statistic\nWhen we estimate the population (grand) mean, there are \\(k-1\\) independent pieces of information\nWhen we estimate the population means within each group, there are \\(n - k\\) independent pieces of information\n\n\nF-Statistic\nThe whole goal of Anova is to compare the variability between groups vs the variability within groups.\n\n\n\nTukeys HSD\nThe main idea of the hsd is to compute the honestly significant difference (hsd) between all pairwise comparisons\n\nType 1 Error\n\\(\\alpha\\) is our significance level, and our Type I error rate.\n\nA Type I error is the risk rejecting the null hypothesis when the null hypothesis is actually true.\n\nAs an example when \\(\\alpha=0.05\\) that means you have a 5% risk of rejecting the null hypothesis when it is actually true.\n\n\n\n\nFamily-Wise Error Rate\nFamily-wise error rate (FWER) is the probability/risk of making one or more Type I error across all tests in a family of comparisons. It is a way to quantify the risk of making Type I errors during multiple hypothesis testing.\nThe formula is,\n\\[\nFWER = 1 - (1-\\alpha)^m\n\\]\nWhere \\(m\\) is the number of comparisons.\n\\[\n\\text{k choose 2} = \\frac{k(k-1)}{2}\n\\]\n\n\nPairwise Comparisons\nCommonly we report confidence intervals to estimate which means are actually different (also reports p-values)\nTukey’s HSD can be represented as,\n\\[\n\\bar{x}_1-\\bar{x}_2 \\pm q^**\\sqrt{MSE*\\frac{1}{n_j}+\\frac{1}{n_{j}^*}}\n\\]\nWhere:\n\n\\(q^*\\) is a value from the studentized range distribution\n(MSE) refers to the average of the squared differences between individual data points within each group and their respective group mean, divided by the degrees of freedom. Also know as the estimated variance by pooling all the groups.\n\n\\[\nMSE = \\sum^k_{i=1}\\frac{(n_i-1)s^2_i}{N-k}\n\\]\n\nHow to find \\(q^*\\)\nCan be found using the qtukey() function\n\nqtukey(p = 0.95, nmeans = 3, df = 27)\n\n[1] 3.506426\n\n\n\n\n\nTest-statistic\n\\[\nQ = \\frac{\\bar{x}_i-\\bar{x}_j}{\\sqrt{\\frac{MSE}{2}(\\frac{1}{n_i}+\\frac{1}{n_j})}}\n\\]\n\n\nSummary\nTukey’s Honest Significant Difference (HSD) test is a post hoc test commonly used to assess the significance of differences between pairs of group means. Tukey HSD is often a follow up to one-way ANOVA, when the F-test has revealed the existence of a significant difference between some of the tested groups.\n\n\n\nBonferroni\nTukey’s HSD is best for making all possible pairwise comparisons, while Bonferroni Correction is more general and suited for a small number of pre-planned comparisons.\nYou conduct individual t-tests, but divide by the original significance level \\(\\alpha\\) by the number of tests being performed (\\(m\\))"
  },
  {
    "objectID": "ST_511_Notes.html#chi-square-test",
    "href": "ST_511_Notes.html#chi-square-test",
    "title": "ST 511 Notes",
    "section": "Chi-Square Test",
    "text": "Chi-Square Test\nThe big idea is to compare expected counts (the assumed counts under the null hypothesis) with the observed counts (your results). This is used to calculate the test statistic.\n\nHypothesis Notation\nHo: Sex and species of penguins are independent\nHa: sex and species of penguins are not independent\nRemember for the null this means species has no impact OR species is completely independent from sex. While for the alternative this means species has some impact on sex OR species is not independent from sex.\n\nAssumptions\n\nIndependence\n\n\n\nExpected Frequencies\n\nExpected cell count should be &gt; 5\nIf any of the expected values are low, the p-values generated by the test start to lose reliability. Should check this in our table.\n\n\n\n\n\nChi-Square Distribution\nBinomial Approximation: The observed Counts in a contingency table follow a binomial distribution\nThe Central limit theorem tells us that when the number of trials (\\(n\\)) is large enough, the binomial distribution can be reasonably approximated by the normal distribution\nThe Chi-Square test relies on the assumption that the sampling distribution of the observed frequencies (counts) is close enough to a normal distribution for the \\(X^2\\) statistic to accurately follow the theoretical Chi-Square distribution\nWhen Expected Counts are high (\\(\\geq5\\)): The underlying discrete (integer) count data behaves smoothly, like a continuous bell-shapped (Normal) curve\nWhen Expected Counts are low (\\(&lt;5\\)): The data distribution remains highly skewed and chunky (discrete). It looks nothing like the smooth Normal distribution.\nIf our sample size assumption (expected counts) is not met, the test statistic doesn’t actually follow the distribution it’s being compared to…. so we can’t trust the results.\n\n\nExpected Counts\nTo calculate the expected count for the \\(i^{th}\\) row and \\(j^{th}\\) column,\n\\[\n\\text{Expected Count}_{\\text{row i, col j}} = \\frac{\\text{(row i total)}*\\text{(col j total)}}{\\text{table total}}\n\\]\n\n\n\n\nTest Statistic\n\n\n\\(r\\) is the number of groups for one variable (rows)\n\\(c\\) is the number of groups for the second variable (columns)\nThe test statistic cannot be negative\n\n\np-value\nTo calculate the p-value you can use chisq_test(data, var1 ~ var2)\n\n\nNotation\nThe probability of observing our statistic, or something larger given the null hypothesis is true.\n\n\\(P(\\chi^2 \\geq X^2) = \\text{p-value}\\)\n\n\\(\\chi^2\\) = map of possible outcomes on the chi-square distribution (random variable because it is generated from a random process)\n\\(X^2\\) = your statistic\n\n\n\n\n\nImportant Notes\n\nThe shape of the Chi-Square distribution curve depends on the degrees of freedom\nThe chi-square distribution is skewed to the right, and the chi-square test is always right-tailed\nA chi-square test cannot have a directional hypothesis. A chi-square value can only indicate that a relationship exists between two variables, not what the relationship is.\n\n\n\nChi-Square Difference in Proportions\n\n\nZ-Statistic\n\\[\nZ = \\frac{(\\hat{p}_1-\\hat{p}_2)}{SE_{pooled}} = \\frac{(0.5385 -0.3684)}{0.1786} = \\frac{0.1701}{0.1786} \\approx 0.952\n\\]\n\\[\n.952^2 = .906\n\\]\n\ncounts_table &lt;- matrix(c(7,7,6,12), nrow = 2)\n\nprop.test(counts_table, correct = F)\n\n\n    2-sample test for equality of proportions without continuity correction\n\ndata:  counts_table\nX-squared = 0.90688, df = 1, p-value = 0.3409\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.177065  0.517146\nsample estimates:\n   prop 1    prop 2 \n0.5384615 0.3684211"
  },
  {
    "objectID": "ST_511_Notes.html#linear-regression",
    "href": "ST_511_Notes.html#linear-regression",
    "title": "ST 511 Notes",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nSummary Statistics\nCorrelation (r):\n\nIs bounded between [-1, 1]\nMeasures the strength + direction of a linear relationship\nThe correlation value has no units and is not impacted by a linear change in units (e.g. going from inches to centimeters)\n\n\nSlope + intercept (fit a line)\n\n\nSyntax\ncor(x, y)\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ purrr     1.0.2\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nairquality |&gt;\n  summarise(corr = cor(Wind, Temp, use = \"complete.obs\"))\n\n        corr\n1 -0.4579879\n\n\n\nairquality |&gt;\n  ggplot(aes(x = Wind, y = Temp)) +\n  geom_point() +\n  geom_smooth(method = 'lm', se = F, formula = 'y ~ x')\n\n\n\n\n\n\n\n\n\n\nResidual\nThe residual of the \\(i^{th}\\) observation \\((x_i,y_i)\\) is the difference of the observed outcome and the outcome we would predict based on the model fit:\n\\[e_i = y_i-\\hat{y}_i\\]\n\n\\(y_i\\) is an observed value.\n\\(\\hat{y}_i\\) is the predicted value based on the line. It is typically found by plugging \\(x_i\\) into the model.\n\nTo minimize the residual sum of squares:\n\\[\n\\sum(y_i-\\hat{y})^2\n\\]\n\nResidual Overlay Plot:\n\n# Fitting a linear model\nair_lm &lt;- lm(Wind ~ Temp, data = airquality) \n\n# Adding fitted and residuals \nairquality$fit  &lt;- fitted(air_lm)\nairquality$res  &lt;- resid(air_lm)\n\n# Creating residual overlay plot\nggplot(airquality, aes(Temp, Wind)) +\n  geom_segment(aes(xend = Temp, yend = fit), alpha = 0.6) + \n  geom_point() +\n  geom_smooth(method = \"lm\", se = F, formula = y ~ x, color = \"purple\") +\n  labs(title = \"Residuals\", y = \"Wind\", x = \"Temp\")\n\n\n\n\n\n\n\n\n\n\n\nLinear Regression Equation\nPopulation level: \\(y=\\beta_0+\\beta_1*x+\\epsilon\\)\nSample: \\(\\hat{y}=\\hat{\\beta_0}+\\hat{\\beta_1}*x\\)\n\n\\(\\hat{y}\\) (yhat) = predicted value of y\n\\(\\hat{\\beta_0}\\) (b) = estimated intercept\n\\(\\hat{\\beta_1}\\) (b1) = estimated slope\n\\(x\\) = explanatory variable\n\nThe slope describes the estimated difference in the predicted average outcome of \\(y\\) if the predictor variable \\(x\\) happened to be one unit larger. The intercept describes the average outcome of \\(y\\) if \\(x\\) = 0 and the linear model is valid all the way to \\(x\\) = 0 (values of \\(x\\) = 0 are not observed or relevant in many applications)\n\n\nCategorical Predictors\nThe estimated intercept is the value of the outcome variable for the first category (i.e., the category corresponding to an indicator value of 0). The estimated slope is the average change in the outcome variable between the two categories.\n\n\nCoefficient of Determination\n\\(R^2\\) is also called the coefficient of determination, which is the proportion of variability in the outcome variable explained by the model.\nSince \\(r\\) is always between -1 and 1, \\(R^2\\) will always be between 0 and 1. It measures the proportion of variation in the outcome variable, \\(y\\), that can be explained by the linear model with predictor \\(x\\).\n\n\nSums of Squares\n\n\n\n\nR-Code\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n\n\n✔ broom        1.0.10     ✔ rsample      1.2.1 \n✔ dials        1.3.0      ✔ tune         1.2.1 \n✔ infer        1.0.7      ✔ workflows    1.1.4 \n✔ modeldata    1.4.0      ✔ workflowsets 1.1.0 \n✔ parsnip      1.2.1      ✔ yardstick    1.3.1 \n✔ recipes      1.1.0      \n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use suppressPackageStartupMessages() to eliminate package startup messages\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   23.2      2.11       11.0  4.90e-21\n2 Temp          -0.170    0.0269     -6.33 2.64e- 9\n\n\n\n\nWhy Mean Response\nThe phrase expected value is a synonym for mean value in the long run (meaning for meany repeats or a large sample size)\n\n\n\nNull and Alternative Hypothesis\n\\(H_o:\\beta_1=0\\)\n\\(H_a:\\beta_1\\neq0\\)\n\n\nAssumptions\n\nIndependence\nMost important thing is how you sample the data, with the fundamental idea being that the population you randomly sampled from had an equal chance for selection, and that the observations are independent from one another.\nTo check you will want to plot the residuals:\n\nplot(airquality$Temp, airquality$res,\n     ylab = \"Residuals\",\n     xlab = \"Temperature\",\n     main = \"Wind Speed Residuals\"\n     )\nabline(0, 0)\n\n\n\n\n\n\n\n\nThe residuals should be randomly and symmetrically distributed around zero under all conditions, and in particular there should be no correlation between consecutive errors no matter how the rows are sorted, as long as it is on some criterion that does not involve the dependent variable. If this is not true, it could be due to a violation of the linearity assumption or due to bias that is explainable by omitted variables (say, interaction terms or dummies for identifiable conditions). Source\n\n\nLinearity\nThe first step is to look at a scatter plot of the data, to see if there is a clear violation of linearity. If you don’t notice a clear polynomial or logistic regression, that is a good sign of a linear relationship.\nNon-linearity is most evident in a plot of residual vs. fitted (predicted) values. The points should be symmetrically distributed around a horizontal line, with a roughly constant variance. The points should appear random rather than following a specific pattern. What you are checking for is equal variance.\n\nA violation of linearity would appear to show a clear bowed pattern:\n\n\n\nEqual Variance\nViolations of homoscedasticity (which are called “heteroscedasticity”) make it difficult to gauge the true standard deviation of the forecast errors, usually resulting in confidence intervals that are too wide or too narrow, and creates inaccurate p-values. Heteroscedasticity may also have the effect of giving too much weight to a small subset of the data (namely the subset where the error variance was largest) when estimating coefficients.\nYou do not want to see the residual plot showing unequal spread of data, if there is a clear pattern as in the plot below, that is a good sign the data does not have equal variance.\n\n\n\nLinearity and Equal Variance Summary\n\n\n\nNormality\nWe think about our sample size and also look at Normal Q-Q plots of our residuals. The residuals should roughly follow the straight line mapped by the plot, deviations would indicate the data may not meet the normality assumption. If the sample size is large the CLT would account for minor deviations, but if your sample size is small minor deviations should be carefully evaluated.\n\nairquality |&gt;\n  ggplot(aes(sample=res)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\") +\n  labs(\n    title = \"Normal Q-Q Plot of Residuals\",\n    subtitle = \"Model: Wind ~ Temp (Airquality Data\",\n    x = \"Theoretical Quantiles (Standard Normal)\",\n    y = \"Sample Quantiles (Residuals)\"\n  )\n\n\n\n\n\n\n\n\nA faster way to make plots for your linear regression is to just use the built in plot() function on your model:\n\nplot(air_lm)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTest Statistic\n\\[\nt = \\frac{\\hat{\\beta_1}-\\beta_{null}}{SE(\\hat{\\beta_1})}\n\\]\n\nStandard Error\n\n\n\n\nConfidence Intervals and Predictions\nHow to predict true mean Wind (mph) when the Temp is 17 degrees? Note that a Temp of 17 is outside the bounds of our data, meaning we are extrapolating.\n\nResponse\n\\[\n\\hat{\\mu_y} \\pm t^**SE_{\\hat{\\mu_y}}\n\\]\n\n\nStandard Errors\n\\(X_{given}\\) = What we set X equal to\n\nThe term \\(\\frac{1}{n}\\) accounts for the general uncertainty in estimating the intercept.\nThe term \\(\\hat{\\sigma}^2\\) in the outer square root represents the uncertainty around our prediction.\nThe uncertainty introduced by the slope (\\(\\hat{\\beta}_1\\)) is accounted for by,\n\\[\n\\frac{(X_{given}-\\bar{X})^2}{\\sum^n_{i=1}(X_i-\\bar{X})^2}\n\\]\n\n\nR-Code\n\nmodel1 &lt;- lm(Wind ~ Temp, data = airquality)\n\nnew_temp_data &lt;- tibble(Temp = 17)\n\npredict(\n  model1, \n  newdata = new_temp_data, \n  interval = \"confidence\", \n  level = 0.95\n)\n\n       fit      lwr      upr\n1 20.33579 17.05817 23.61342"
  },
  {
    "objectID": "ST_511_Notes.html#multiple-linear-regression",
    "href": "ST_511_Notes.html#multiple-linear-regression",
    "title": "ST 511 Notes",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\n\nHas a quantitative response (Y)\nHas &gt; 1 explanatory variable\nThe values of the coefficients change when we add more variables into our model\n\n\nAdditive Model\nThe relationship between x and y does not depend on z\n\nlibrary(palmerpenguins)\n\n\nAttaching package: 'palmerpenguins'\n\n\nThe following object is masked from 'package:modeldata':\n\n    penguins\n\npenguins_clean &lt;- penguins |&gt;\n  filter(bill_length_mm != is.na(bill_length_mm),\n         flipper_length_mm != is.na(flipper_length_mm))\n\n# Creating additive model\nadd_model &lt;- lm(flipper_length_mm ~ bill_length_mm + species, data = penguins_clean)\n\n# Grabbing predictions\npenguins_clean$pred_flipper &lt;- predict(add_model)\n\n# Creating new data to assign full x scale to\nx_scale_df &lt;- expand.grid(\n  bill_length_mm = seq(\n    from = min(penguins_clean$bill_length_mm),\n    to = max(penguins_clean$bill_length_mm),\n    by = .1, # Sets the scale increments\n  ),\n  species = unique(penguins_clean$species)\n)\n                            \n                          \n\n# adding new prediction lines\nx_scale_df$predline &lt;- predict(add_model, x_scale_df)\n\nggplot(data = penguins_clean, aes(x = bill_length_mm, y = flipper_length_mm, color = species)) +\n  geom_point(alpha = 0.5) +\n  geom_line(data = x_scale_df, aes(y = predline), size = 1) +\n  labs(\n    title = \"Additive Model\",\n    subtitle = \"Model: Flipper Length ~ Bill Length + Species (Same Slope, Different Intercepts)\",\n    x = \"Bill Length (mm)\",\n    y = \"Flipper Length (mm)\"\n  ) +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\nModel Output\n\ntidy(add_model)\n\n# A tibble: 4 × 5\n  term             estimate std.error statistic   p.value\n  &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)        148.       4.17      35.4  6.78e-116\n2 bill_length_mm       1.08     0.107     10.1  3.12e- 21\n3 speciesChinstrap    -5.00     1.37      -3.65 3.00e-  4\n4 speciesGentoo       17.8      1.17      15.2  3.45e- 40\n\n\n\n\nGeneral Formula\n\\[\n\\hat{Y} = \\hat{\\beta_0} + \\hat{\\beta}_1X_1 + \\hat{\\beta}_2X_2 + \\text{...} + + \\hat{\\beta}_kX_k\n\\]\nOur formula in our example would be,\n\\[\n\\hat{fl} = 147.951 + 1.083 * \\text{bl} - 5.004 * \\text{chin} + 17.799 * \\text{gentoo}\n\\]\n\\[\nI_{Gentoo} \\{\\text{1 when Gentoo, 0 for all else}\n\\]\n\\[\nI_{Chin} \\{\\text{1 when Chin, 0 for all else}\n\\]\n\n147.95 represents the intercept for Adelie penguins which is our baseline reference, the formula just looking at the Adelie penguins would look like,\n\n\\[\n\\hat{fl}_{Adelie} = 147.951 + 1.083 * \\text{bill}\n\\]\n\nTo represent just the formula for Chinstrap penguins we could use,\n\n\\[\n\\hat{fl}_{Chin} = 147.951  - 5.004 + 1.083 * \\text{bill}\n\\]\n\n\nInterpretations\nHolding species constant, for a 1mm increase in bill length, we estimate an average increase in flipper length of 1.083mm.\nHolding bill length constant, we estimate the mean flipper length of Gentoo penguins to be 17.799mm larger than the Adelie penguins.\n\n\n\nInteraction Model\nThe relationship between x and y depends on the value of z.\n\ninter_model &lt;- lm(flipper_length_mm ~ bill_length_mm * species, data = penguins_clean)\n\npenguins_clean |&gt;\n  ggplot(\n    aes(x = bill_length_mm, y = flipper_length_mm, color = species)\n  ) + \n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = F, formula = \"y ~ x\") +\n  labs(\n    title = \"Interaction Model\",\n    subtitle = \"Flipper Length ~ Bill Length * Species (Non-Parallel Slopes)\",\n    x = \"Bill Length (mm)\",\n    y = \"Flipper Length (mm)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nOutput\n\ntidy(inter_model)\n\n# A tibble: 6 × 5\n  term                            estimate std.error statistic  p.value\n  &lt;chr&gt;                              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)                      159.        6.90     23.0   4.72e-71\n2 bill_length_mm                     0.800     0.178     4.50  9.17e- 6\n3 speciesChinstrap                 -12.3      12.5      -0.986 3.25e- 1\n4 speciesGentoo                     -7.83     10.6      -0.736 4.63e- 1\n5 bill_length_mm:speciesChinstrap    0.207     0.276     0.750 4.54e- 1\n6 bill_length_mm:speciesGentoo       0.591     0.246     2.40  1.67e- 2\n\n\nHow would we interpret something like the 0.207 value?\nFor a 1mm increase in bill length, we estimate an average increase in flipper length of 0.207mm more for Chinstrap than Adelie penguins, holding all other variables constant.\n\n\nFormula\n\\[\n\\hat{Y} = \\hat{\\beta_0} + \\hat{\\beta}_1X_1 + \\hat{\\beta}_2X_2 + \\hat{\\beta}_3(X_1X_2)\n\\]\nFor our example the formula would look like,\n\\[\n\\hat{fl} = 158.92  + 0.80 *bill - 12.29 * Chin - 7.83 * Gentoo + .207 * bill * Chin + .591 * bill * Gen\n\\]\nThe line for just Adelie penguins would look like,\n\\[\n\\hat{fl}_{Adelie} = 159 + .800 * bill\n\\]\nThe line for just Chinstrap penguins would look like,\n\\[\n\\hat{fl}_{Chin} = 159 - 12.29 + .800 * bill + .207 * bill\n\\] - \\(159 - 12.29\\) is the change to our intercept in the model while \\(bill(.800 + .207)\\) would represent our slope\n\n\nAssumptions\nWe use the same assumptions as the assumptions we used for singular linear regression models, but add another assumption for multicollinearity.\nMulticollinearity happens when the predictor variables are correlated within themselves. When the predictor variables themselves are correlated, the coefficients in a multiple regression model can be difficult to interpret.\n\nMulticollinearity Example\nThis is an example using fake data:\n\n\nThe photos show how the variables react in a linear regression model on their own.\n\nTO check this assumption you can calculate correlation coefficients between all your predictors. You can also calculate variance inflation factors (not covered in course)\n\n\n\n\nHypothesis Test for Multiple Linear Regression\nTo illustrate this lets first look back at our examples for our simple linear regression compared to our additive multiple linear regression. Represented by the formulas,\n\\[\nfl = \\beta_0 + \\beta_1 * bill + \\epsilon\n\\]\n\\[\nfl = \\beta_0 + \\beta_1 * bill + \\beta_2 * Chin + \\beta_3  * Gentoo + \\epsilon\n\\]\nThe question is to try and determine if the multiple linear regression model is worth it. One thing to remember is that you are never going to get worse at predicting your response variable by adding extra terms, you are merely checking if adding those extra terms are worth it.\n\nNotation\n\\(H_0: \\beta_2 = \\beta_3 = 0\\)\n\\(H_a: \\text{At least one of the betas differ from zero}\\)\nThe idea is to see if the reduction in unexplained variance (the reduction in Residual Sum of Squares, RSS) when moving from \\(Model_{reduced}\\) to \\(Model_{full}\\) is statistically significant. In other words, is species a good predictor to include in our analysis.\n\n\nTest Statistic\n\\[\nF = \\frac{(RSS_R-RSS_f)/(df_R-df_F)}{MSE_F}\n\\]\n\\(df_F = n - k_F -1\\): n is our sample size, while k is the number of predictors\n\\(df_R = n - k_R -1\\): n is our sample size, while k is the number of predictors\n\nF represents our full model, while R represents our reduced model.\n\n\nVisualization\nWe are comparing the Residual Sum of Squares between the two models\nReduced Model:\n\n# Fitting a linear model\npeng_slm &lt;- lm(flipper_length_mm ~ bill_length_mm, data = penguins_clean) \n\n# Creating clone df\npeng_clone &lt;- penguins_clean\n\n# Adding fitted and residuals \npeng_clone$fit  &lt;- fitted(peng_slm)\npeng_clone$res  &lt;- resid(peng_slm)\n\n# Creating residual overlay plot\nggplot(peng_clone, aes(x = bill_length_mm, y = flipper_length_mm)) +\n  geom_segment(\n    aes(xend = bill_length_mm, yend = fit), \n    color = 'black',\n    linetype = 'dotted',\n    alpha = 0.5\n  ) + \n  geom_smooth(method = \"lm\", se = F, formula = y ~ x, color = \"purple\") +\n  geom_point(color = 'darkorange', alpha = 0.6) +\n  labs(\n    title = \"Residual Distances for the Reduced Model\", \n    subtitle = \"Model: Flipper Length ~ Bill Length. Vertical lines show model error (residuals)\",\n    x = \"Bill Length (mm)\", \n    y = \"Flipper Length (mm)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFull Model:\n\n# Fitting a linear model\npeng_flm &lt;- lm(flipper_length_mm ~ bill_length_mm + species, data = penguins_clean) \n\n# Creating clone df\npeng_clone1 &lt;- penguins_clean\n\n# Adding fitted observations to df\npeng_clone1$fit  &lt;- fitted(peng_flm)\n\n# Creating residual overlay plot\nggplot(peng_clone1, aes(x = bill_length_mm, y = flipper_length_mm, color = species)) +\n  geom_segment( \n    aes(xend = bill_length_mm, yend = fit), # draws a line from observed points to fitted value\n    color = \"black\",\n    linetype = 'dotted',\n    alpha = 0.5\n  ) +\n  geom_point(alpha = 0.6) +\n  geom_line(aes(y = fit), size = 1) + # y = fit is the values created from the linear model\n  labs(\n    title = \"Residual Distances for the Full Additive Model (Forced Parallel Lines)\", \n    subtitle = \"Model: Flipper Length ~ Bill Length + Species\",\n    x = \"Bill Length (mm)\", \n    y = \"Flipper Length (mm)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nYou can compare the length of the distance between residuals and how that compares between a singular line versus multiple lines.\n\n\n\nAssumptions\nYou need to check for both models:\n\nIndependence\n\nSame as with simple linear regression you just want to make sure it is reasonable your data is independent. You will want a simple random sample\n\nLinearity\nNormality of Residuals\nEqual Variance\nNested models\n\n\nLinearity and Equal Variance\n\n# Adding residuals to df\npeng_clone1$res &lt;- resid(peng_flm) \n\n# residual plot vs fitted\nggplot(data = peng_clone1, aes(x = fit, y = res)) +\n  geom_point(alpha = 0.6) +\n  geom_hline(\n    yintercept = 0,\n    color = 'red',\n    linetype = 'dashed',\n    size = 1\n  ) +\n  geom_smooth(\n    method = 'loess',\n    se = F,\n    color = 'blue',\n    size = 1\n  ) +\n  labs(\n    title = \"Residuals vs Fitted Plot\",\n    subtitle = \"Model: Flipper Length ~ Species + Bill Length\",\n    x = \"Fitted Values (Predicted Flipper Length)\",\n    y = \"Residuals (Error)\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nBased on this plot we can see that the residuals are fairly randomly distributed around zero indicating the data meets the linearity assumption. For equal variance we want to see the data is spread across the plot without creating an imbalance between any specific part along the x-axis. Since the residuals are not fanning out or funneling into to any specific point, we can state the model meets the assumption of equal variance.\n\n\nNormality\n\npeng_clone1 |&gt;\n  ggplot(aes(sample = res)) +\n  stat_qq(color = 'darkgreen') +\n  stat_qq_line(color = 'black', size = 1) +\n  labs(\n    title = \"Normal Q-Q Plot of Residuals for Flipper Length Model\",\n    subtitle = \"Model: Flipper Length ~ Species + Bill Length\",\n    x = \"Theoretical Quantiles (Normal Distribution)\",\n    y = \"Sample Quantiles (Model Residuals)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nSince we have a large sample size we are more robust to deviations of normality, so we can be robust to the outlier seen on the left tail of our plot.\n\nplot(peng_flm)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNested Models\nYour reduced model has to be inside your full model, otherwise it will create negative values that will impact the ability to calculate your test statistic.\n\n\n\nR-Code for Anova for Comparisons\n\nmodel_reduced &lt;- lm(flipper_length_mm ~ bill_length_mm, data = penguins)\n\nmodel_full &lt;- lm(flipper_length_mm ~ bill_length_mm + species, data = penguins)\n\nanova(model_reduced, model_full)\n\nAnalysis of Variance Table\n\nModel 1: flipper_length_mm ~ bill_length_mm\nModel 2: flipper_length_mm ~ bill_length_mm + species\n  Res.Df   RSS Df Sum of Sq      F    Pr(&gt;F)    \n1    340 38394                                  \n2    338 11471  2     26923 396.64 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe DF column difference in residual degrees of freedom between the two adjacent models being compared. This is the numerator degrees of freedom for the F-test. The denominator degrees of freedom is the Residual degrees of freedom from the full model (338).\nThe 26923 is the reduction in the Residual Sum of Squares (RSS) from the reduced to full model.\nOur p-value comes from a F-distribution with 2 and 338 degrees of freedom.\n\nInterpretation\nSince our p-value is really small we can reject the null hypothesis, and state that the amount of variation in flipper length explained by the addition of species is statistically significant.\n\n\n\nAdditive vs Interaction Model\nAdditive:\n\\[\nfl = \\beta_0 + \\beta_1 * bill + \\beta_2 * Chin + \\beta_3  * Gentoo + \\epsilon\n\\]\nInteraction:\n\\[\nfl = \\beta_0 + \\beta_1 * bill + \\beta_2 * Chin + \\beta_3  * Gentoo + \\beta_4*bill*Chin + \\beta_5 * bill * Gentoo + \\epsilon\n\\]\nSince we previously plotted our residuals for our additive model lets see what it would look like on our interaction model\n\n# Fitting a linear model\npeng_flm &lt;- lm(flipper_length_mm ~ bill_length_mm * species, data = penguins_clean) \n\n# Creating clone df\npeng_clone1 &lt;- penguins_clean\n\n# Adding fitted observations to df\npeng_clone1$fit  &lt;- fitted(peng_flm)\n\n# Creating residual overlay plot\nggplot(peng_clone1, aes(x = bill_length_mm, y = flipper_length_mm, color = species)) +\n  geom_segment( \n    aes(xend = bill_length_mm, yend = fit), # draws a line from observed points to fitted value\n    color = \"black\",\n    linetype = 'dotted',\n    alpha = 0.5\n  ) +\n  geom_point(alpha = 0.6) +\n  geom_line(aes(y = fit), size = 1) + # y = fit is the values created from the linear model\n  labs(\n    title = \"Residuals for the Full Interaction Model\", \n    subtitle = \"Fitted Lines: Flipper Length ~ Bill Length * Species (Different Slopes and Intercepts\",\n    x = \"Bill Length (mm)\", \n    y = \"Flipper Length (mm)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nAnova Comparison\n\nmodel_additive &lt;- lm(flipper_length_mm ~ bill_length_mm + species, data = penguins_clean)\n\nmodel_interaction &lt;- lm(flipper_length_mm ~ bill_length_mm * species, data = penguins_clean)\n\nanova(model_additive, model_interaction)\n\nAnalysis of Variance Table\n\nModel 1: flipper_length_mm ~ bill_length_mm + species\nModel 2: flipper_length_mm ~ bill_length_mm * species\n  Res.Df   RSS Df Sum of Sq      F  Pr(&gt;F)  \n1    338 11471                              \n2    336 11272  2    199.66 2.9759 0.05235 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nHypothesis Notation\n\\(H_0: \\beta_4 = \\beta_5 = 0\\)\n\\(H_A:\\) At least one of \\(\\beta_4 \\ \\text{or} \\ \\beta_5 \\ne 0\\)\nAt a significance level of \\(\\alpha = 0.05\\) and a p-value of 0.0523 we fail to reject the null hypothesis. Due to the closeness to the \\(\\alpha\\) level we can state there is weak evidence that the addition of interaction effect between bill length and species better explains the variation in flipper length over our Additive model."
  },
  {
    "objectID": "ST_511_Notes.html#statistical-inference-vs-sampling-variability",
    "href": "ST_511_Notes.html#statistical-inference-vs-sampling-variability",
    "title": "ST 511 Notes",
    "section": "Statistical Inference vs Sampling Variability",
    "text": "Statistical Inference vs Sampling Variability\nStatistical Inference - taking a sample, and making claims about a larger population.\nSampling Variability - the natural variability (error) we expect/see from sample to sample.\n\nEx: Let’s assume that Gen-Z has a mean credit score of 680 (μ=680). If you take a random sample of 100 Gen-Z individuals, would we expect the mean credit score of those 100 to be exactly 680?\n\nNo, we would not assume it be the exact same mean. Because we know this, we account for sampling variability by using distributions (instead of just point estimates) when trying to make claims about an entire population (Stastical Inference)!\n\n\n\nScenarios\nIt’s important for us to know when methods are vs are not appropriate, based on variable type. We are going to go through three examples and identify each scenario on which methods would be most appropriate to answer the research question.\n\nExample 1\nAn ecologist is studying a population of small mammals (e.g., field mice) to see if their primary habitat is associated with their dominant predator avoidance tactic. The study aims to determine if animals that live in dense forest prefer to hide, while those in open fields prefer to flee.\n\n\nThis would use a Chi-Square Distribution, thus statistical inference is used.\n\n\n\nExample 2\nWhat was the median student loan debt for all 350 students who graduated from the Engineering School in May 2025?\n\nThis would not need any form of statistical inference as your population of interest would be all 350 students, thus sampling variability does not need to be accounted for.\n\n\n\nExample 3\nA pharmacologist is studying the factors that influence patient Recovery Time (in days) following a specific surgical procedure. They are analyzing the effects of a new post-operative drug and the patient’s age.\nThe researcher hypothesizes a significant interaction between the two factors: the new drug may be highly effective (significantly reducing recovery time) for younger patients, but its efficacy might be greatly reduced or even disappear for older patients due to metabolic changes.\n\nSince this would be looking at a categorical explanatory variable (new drug or no new drug) with an interaction effect using a quantitative explanatory variable (age of patient) analyze a quantitative response variable (recovery time) the best option would be to conduct statistical inference using regression."
  },
  {
    "objectID": "ST_511_Notes.html#logistic-regression",
    "href": "ST_511_Notes.html#logistic-regression",
    "title": "ST 511 Notes",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nLogistic regression is a modeling tool used for a categorical response variable. Is very similar to linear regression, but obviously the response variable is different.\n\nThe Problem\n\\((\\mu_y) = (\\beta_0 + \\beta_1X_1 + ...)\\)\n\nThe right side of a regression equation, which cant take any value from \\(-\\infty\\) to \\(\\infty\\).\n\n\\((\\mu)\\): The expected value of the response variable, which is restricted to a certain range (e.g. a probability must be between 0 and 1).\nApplying a specific mathematical transformation to the expected outcome (or mean) of your model in order to linearly relate it to your predictor variables, while restricting the values of your response variable accordingly.\nFor logistic regression, this is the logit-link function:\n\\((\\mu_y \\ or \\ p)\\)\n\\(ln(\\frac{p}{1-p})\\) = log - odds\n\n\\(p\\) = probability of success\n\\(1-p\\) = probability of failure\nodds = \\(\\frac{p}{1-p}\\)\n\n\\(\\widehat{ln(\\frac{p}{1-p})} = \\hat{\\beta}_0 +\\hat{\\beta}_1X_1+...\\)\n\n\nGeneralized Linear Model (GLM)\nGLM is another term for a type of logistic regression model. Where we want to fit an S curve and model the probablity of success as a function of explanatory variable(s).\n\n\nTerms\nBernoulli Distribution:\n\n2 outcomes: Success (p) or Failure (1-p)\n\\(y_i\\) ~ Bern(p)\nWe use \\(p_i\\) for estimated probabilities\n\n\n\nThe model\n\\[\n\\widehat{ln(\\frac{p}{1-p})} = \\hat{\\beta_o} + \\hat{\\beta_1}X_1 + ...\n\\]\n\n\\(ln(\\frac{p}{1-p})\\) is called the logit link function, and can take on values from \\(-\\infty\\) to \\(\\infty\\)\n\\(ln(\\frac{p}{1-p})\\) represents the log odds of a success\n\n\nLog Odds\n\\(ln(\\frac{p}{1-p})\\) = natural logarithm of the odds, where the odds are the ratio of the probability of an event happening to the probability of it not happening.\n\\(ln(\\frac{p}{1-p})\\) has good modeling properties (ex. unbounded; handles extreme values of p well for linearity; restricts p to be between 0 and 1), but it is hard to interpret. Think of this as our initial model, that we will work with to model probabilities (can also model odds).\n\n\\(p\\) stands for probability\nThe logit link function restricts p to be between the values [0, 1]\n\n\n\n\nMath\nWe want to model probabilities. We want to isolate \\(p\\) on the left side of the equation,\n\\[\n\\widehat{ln(\\frac{p}{1-p})} = \\hat{\\beta_o} + \\hat{\\beta_1}X_1 + ...\n\\]\nTo isolate the \\(p\\) we get rid of the \\(ln\\) by,\n\\[\ne^{ln(\\frac{p}{1-p})} = e^{\\beta_0+\\beta_1X1}\n\\]\n\\[\n\\frac{p}{1-p} = e^{\\beta_0+\\beta_1X1}\n\\]\nTimes each side by \\((1-p)\\),\n\\[\np = e^{\\beta_0+\\beta_1X1} * (1-p)\n\\]\n\\[\np = e^{\\beta_0+\\beta_1X1} - pe^{\\beta_0+\\beta_1X1}\n\\]\nAdd the new \\(p*e\\) term to the other side,\n\\[\np +pe^{\\beta_0+\\beta_1X1} = e^{\\beta_0+\\beta_1X1}\n\\]\nSimplify the equation to just one p,\n\\[\np(1+e^{\\beta_0+\\beta_1X1}) = e^{\\beta_0+\\beta_1X1}\n\\]\nDivide both side by the \\((1+e)\\) term,\n\\[\np = \\frac{e^{\\beta_0+\\beta_1X1}}{1+e^{\\beta_0+\\beta_1X1}} \\ \\text{where} \\ p = [0,1]\n\\]\n\nFinal Model\n\\[\n\\hat{p} = \\frac{e^{\\hat{\\beta_0}+\\hat{\\beta_1}X1}}{1+e^{\\hat{\\beta_0}+\\hat{\\beta_1}X1}} \\ \\text{where} \\ p = [0,1]\n\\]\n\n\n\nRecap\nWith a categorical response variable, we use the logit link (logistic function) to calculate the log odds of a success,\n\\[\n\\widehat{ln(\\frac{p}{1-p})} = \\hat{\\beta_o} + \\hat{\\beta_1}X_1 + ...\n\\]\nWe can use the same model to estimate the probability of a success,\n\\[\n\\hat{p} = \\frac{e^{\\hat{\\beta_0}+\\hat{\\beta_1}X1}}{1+e^{\\hat{\\beta_0}+\\hat{\\beta_1}X1}} \\ \\text{where} \\ p = [0,1]\n\\]\n\n\n\nPrediction Interval\n\\[\n\\hat{p} \\pm ME\n\\]\n\\(ME = t^* * SE(\\hat{p})\\)"
  }
]